---
title: "Robbins-Monro算法(RM 算法)"
date: 2025-12-02T14:47:48+08:00
categories: ["Reinforcement Learning"]
tags: ["Robbins-Monro"]
---
## Robbins-Monro算法(RM 算法)

RM算法（Robbins-Monro）是一个**求根算法**，它的设计初衷是专门用来寻找让某个函数等于 **0** 的点。

在  **随机逼近（Stochastic Approximation）** 领域，RM算法指的是  **Robbins-Monro 算法**。它是现代随机优化（包括深度学习中的SGD）的理论基石。

#### 1. 它解决什么问题？

它的核心目的是：**在有噪声（随机误差）干扰的情况下，找到一个函数的根（即零点）。**

- **常规场景（无噪声）：** 假设你有一个方程 $f(x)=0$，你想求 $x$。如果能准确计算$ f(x)$ 和它的导数，我们可以用牛顿法快速求解。
- **RM场景（有噪声）：** 现实中，我们无法直接得到准确的 $ f(x)$ ，每次测量都会带有一个随机噪声$\eta$ 。也就是说，我们观测到的是 $y=f(x)+\eta$。如果用常规方法，噪声会让计算结果剧烈震荡，无法收敛。

#### 2. 算法核心公式

RM算法提出了一种迭代方法，通过不断修正猜测值来逼近真实解：

$w_{n+1}=w_n−a_ny_n$

- $w_n$：第 $n$次对解的猜测。
- $y_n$：在 $w_n$ 处观测到的带噪声的函数值。
- $a_n$：**步长（Step Size）**，也常被称为学习率。

**直观理解：**
看观测值$y_n$ 是正还是负，如果是正的，就往反方向减一点；如果是负的，就加一点。

#### 3. 成功的关键：步长序列$a_n$

RM算法最天才的地方在于规定了步长 $a_n$必须满足两个条件，算法才能保证收敛到真值：

1.  $\sum a_n = \infty$：步长加起来要是无穷大。这意味着$a_n$不能快速的收敛到0，如果快速收敛到0了，初始值$w_1$就会离方程的解 $w^*$ 很远就无法收敛到 $w^*$ 了。
2.  $\sum a_n^2 < \infty$：步长的平方和要是有限的。这意味着随着时间推移，步长必须减小得足够快，以抵消噪声的影响，防止在终点附近无限震荡。

*典型例子：* $a_n=1/n$ （随着次数增加，步长越来越小）。

#### 4. 形象的类比

想象你在蒙着眼睛调试水龙头的温度，目标是得到40度温水（$f(x)=T(x)-40$）：

- 你摸了一下水，感觉烫了（观测值 $y$ 有误差，可能是手感不准）。
- 你把开关往回拧一点。
- **关键点：** 刚开始你拧的幅度很大（步长 a 大）；随着感觉越来越接近，你拧的幅度必须**越来越微小**。如果你一直大幅度拧，水温就会忽冷忽热（震荡），永远停不下来。RM算法就是告诉你如何科学地减小“拧”的幅度。

##### (1) 问题的本质

- $x$：水龙头旋转的角度（我们控制的变量）。
- $T(x)$：当旋转到 $x$ 角度时，实际流出来的水温（我们观测到的值）。
- **目标**：我们希望 $T(x)=40$。

##### (2) 为什么要转化成 $f(x)=0$？

RM算法（Robbins-Monro）是一个**求根算法**，它的设计初衷是专门用来寻找让某个函数等于 **0** 的点。

为了使用RM算法，我们需要把“目标是40”这个问题，伪装成“目标是0”的问题。

##### (3) 数学定义

我们定义一个新的函数 $f(x)$，也就是**误差函数**：
$$
f(x) = T(x) - 40
$$
这时候，我们来看看 $f(x)$ 的含义：

* **如果 $f(x) > 0$**（比如 +5）：说明 $T(x)$ 是 45 度。意味着 **“太热了”** ，需要降温。
* **如果 $f(x) < 0$**（比如 -5）：说明 $T(x)$ 是 35 度。意味着 **“太冷了”** ，需要升温。
* **如果 $f(x) = 0$**：说明 $T(x) - 40 = 0$，即 $T(x) = 40$。意味着 **“刚刚好”** 。

**结论：**

找到 $f(x) = 0$ 的解，等同于找到了让水温变成 40 度 的那个旋钮角度。

#### 5. 现代应用（最重要的地位）

**随机梯度下降（SGD）** 是 RM 算法最著名的特例。
在机器学习中，我们想求损失函数的最小值（即导数为0的点）。由于数据是分批（Batch）采样的，计算出的梯度带有“噪声”。SGD 本质上就是应用了 Robbins-Monro 算法来寻找梯度的零点。

* **目标：** 找到损失函数 $L(\theta)$ 的**最低点**。
* **数学性质：** 到了最低点，坡度（梯度）应该是平的，也就是梯度为 **0**。
* **对应 RM 算法：** 这里的 $f(x)$ 就是**梯度（导数）**。
  * 我们要找梯度等于 0 的点（$f(x)=0$）。
  * 所以，RM 算法在这里被用来寻找让“梯度为 0”的参数，从而实现损失最小化。
